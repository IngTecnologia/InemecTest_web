"""
Sistema de validadores para preguntas generadas en InemecTest
Validaci√≥n autom√°tica usando m√∫ltiples expertos especializados
"""

import os
import json
import asyncio
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from openai import OpenAI
import re

from .models import (
    ValidationResult,
    ValidatorType,
    QuestionInProcess,
    QuestionBatch,
    QuestionStatus,
    ProcedureStatus,  # ‚Üê AGREGADO: Faltaba este import
    get_current_timestamp
)
from .config import (
    get_system_message,
    get_validator_config,
    get_enabled_validators,
    get_openai_api_key,
    VALIDATORS_CONFIG,
    VALIDATION_THRESHOLD,
    DEBUG_CONFIG,
    MOCK_RESPONSES,
    GENERATION_CONFIG
)

class QuestionValidator:
    """
    Validador individual para un aspecto espec√≠fico de las preguntas
    """
    
    def __init__(self, validator_type: ValidatorType):
        """
        Inicializar validador espec√≠fico
        
        Args:
            validator_type: Tipo de validador (estructura, tecnico, dificultad, claridad)
        """
        self.validator_type = validator_type
        self.config = get_validator_config(validator_type.value)
        self.system_message = get_system_message(f"validator_{validator_type.value}")
        
        # Inicializar cliente OpenAI solo si no estamos en modo mock
        if not DEBUG_CONFIG["mock_openai_calls"]:
            # CORREGIDO: Usar get_openai_api_key() en lugar de os.getenv()
            api_key = get_openai_api_key()
            if not api_key:
                print(f"‚ö†Ô∏è API Key no configurada para validador {validator_type.value}")
                raise ValueError("OPENAI_API_KEY no est√° configurado")
            
            try:
                self.client = OpenAI(api_key=api_key)
                print(f"‚úÖ Cliente OpenAI inicializado para validador {validator_type.value}")
            except Exception as e:
                print(f"‚ùå Error inicializando cliente OpenAI en validador {validator_type.value}: {e}")
                self.client = None
                # Activar modo mock autom√°ticamente
                DEBUG_CONFIG["mock_openai_calls"] = True
        else:
            self.client = None
        
        print(f"üîç Validador {validator_type.value} inicializado")
        if DEBUG_CONFIG["verbose_logging"]:
            print(f"   - Peso: {self.config['weight']}")
            print(f"   - Cr√≠tico: {self.config['critical']}")
            print(f"   - Timeout: {self.config['timeout']}s")

    def _clean_json_response(self, response: str) -> str:
        """
        Limpiar la respuesta de OpenAI removiendo bloques de c√≥digo markdown
        """
        # Remover bloques de c√≥digo ```json ... ```
        cleaned = re.sub(r'```json\s*\n?(.*?)\n?```', r'\1', response, flags=re.DOTALL)
        
        # Remover bloques de c√≥digo ``` ... ```
        cleaned = re.sub(r'```\s*\n?(.*?)\n?```', r'\1', cleaned, flags=re.DOTALL)
        
        # Limpiar espacios en blanco al inicio y final
        cleaned = cleaned.strip()
        
        print(f"üßπ Limpieza de respuesta para {self.validator_type.value}:")
        print(f"   üì§ Original: {response[:100]}...")
        print(f"   üì• Limpio: {cleaned[:100]}...")
        
        return cleaned

    def _extract_json_manually(self, response: str) -> Dict[str, Any]:
        """
        Extraer JSON manualmente cuando el parsing autom√°tico falla
        """
        print(f"üîß Extracci√≥n manual de JSON para {self.validator_type.value}")
        
        # Patr√≥n 1: Buscar JSON completo con llaves
        json_pattern = r'\{[^{}]*"score"[^{}]*"comment"[^{}]*\}'
        match = re.search(json_pattern, response, re.DOTALL)
        
        if match:
            try:
                json_str = match.group()
                data = json.loads(json_str)
                print(f"‚úÖ JSON extra√≠do con patr√≥n 1: {data}")
                return data
            except:
                pass
        
        # Patr√≥n 2: Extraer score y comment por separado
        score_match = re.search(r'"score"\s*:\s*([01])', response)
        comment_match = re.search(r'"comment"\s*:\s*"([^"]*)"', response)
        
        if score_match:
            score = int(score_match.group(1))
            comment = comment_match.group(1) if comment_match else "Comentario extra√≠do autom√°ticamente"
            
            data = {"score": score, "comment": comment}
            print(f"‚úÖ JSON extra√≠do con patr√≥n 2: {data}")
            return data
        
        # Fallback: crear respuesta por defecto
        print(f"‚ö†Ô∏è No se pudo extraer JSON, usando fallback para {self.validator_type.value}")
        return {"score": 1, "comment": f"Validaci√≥n autom√°tica fallback: {response[:50]}"}

    async def validate_batch(self, batch: QuestionBatch, procedure_text: str) -> List[Dict[str, Any]]:
        """
        Validar un lote completo de preguntas con el procedimiento completo
        
        Args:
            batch: Lote de preguntas a validar
            procedure_text: Texto completo del procedimiento t√©cnico
            
        Returns:
            Lista de diccionarios con puntaje_eX y comentario_eX para cada pregunta
        """
        try:
            print(f"üîç Validando lote {batch.batch_id} con validador {self.validator_type.value}")
            
            # Preparar prompt para el validador con procedimiento completo y todas las preguntas
            batch_prompt = self._prepare_batch_prompt(batch, procedure_text)
            
            # Realizar validaci√≥n
            validation_response = await self._call_validator_api(batch_prompt)
            
            # Limpiar la respuesta antes de parsear JSON
            clean_response = self._clean_json_response(validation_response)
            
            # Parsear respuesta JSON limpia - ahora esperamos un array
            try:
                validation_data = json.loads(clean_response)
                print(f"‚úÖ JSON parseado exitosamente para {self.validator_type.value}")
                
                # Validar que es un array de 5 elementos
                if not isinstance(validation_data, list):
                    raise ValueError(f"Se esperaba un array, se recibi√≥: {type(validation_data)}")
                
                if len(validation_data) != 5:
                    raise ValueError(f"Se esperaban 5 elementos, se recibieron: {len(validation_data)}")
                    
            except json.JSONDecodeError as e:
                print(f"‚ùå Error parseando JSON limpio para {self.validator_type.value}: {e}")
                print(f"   Respuesta limpia era: {clean_response}")
                
                # Crear respuesta de fallback para 5 preguntas
                validation_data = self._create_fallback_batch_response(5)
            
            # Validar estructura de respuesta del batch
            self._validate_batch_response_structure(validation_data)
            
            # Determinar el n√∫mero de evaluador basado en el tipo
            evaluator_num = self._get_evaluator_number()
            
            # Convertir a formato esperado por el sistema
            results = []
            for i, item in enumerate(validation_data):
                result = {
                    f"puntaje_e{evaluator_num}": item.get(f"puntaje_e{evaluator_num}", item.get("score", 1)),
                    f"comentario_e{evaluator_num}": item.get(f"comentario_e{evaluator_num}", item.get("comment", ""))
                }
                results.append(result)
            
            if DEBUG_CONFIG["verbose_logging"]:
                print(f"   ‚úÖ Validaci√≥n de lote completada para {self.validator_type.value}")
                for i, result in enumerate(results):
                    evaluator_key = f"puntaje_e{evaluator_num}"
                    comment_key = f"comentario_e{evaluator_num}"
                    print(f"      Pregunta {i+1}: {evaluator_key}={result[evaluator_key]}, {comment_key}='{result[comment_key][:30]}...'")
            
            return results
            
        except Exception as e:
            print(f"‚ùå Error en validador {self.validator_type.value}: {e}")
            
            # Crear resultados de error para las 5 preguntas
            evaluator_num = self._get_evaluator_number()
            error_results = []
            for i in range(5):
                error_result = {
                    f"puntaje_e{evaluator_num}": 1,  # Asumir v√°lido por defecto
                    f"comentario_e{evaluator_num}": f"Error en validaci√≥n autom√°tica: {str(e)[:50]}"
                }
                error_results.append(error_result)
            
            # Si es un validador cr√≠tico, re-lanzar el error
            if self.config["critical"]:
                raise
            
            return error_results

    def _get_evaluator_number(self) -> int:
        """Obtener n√∫mero de evaluador basado en el tipo de validador"""
        mapping = {
            "estructura": 1,
            "tecnico": 2, 
            "dificultad": 3,
            "claridad": 4
        }
        return mapping.get(self.validator_type.value, 1)

    def _prepare_batch_prompt(self, batch: QuestionBatch, procedure_text: str) -> str:
        """
        Preparar el prompt con el procedimiento completo y el lote de preguntas
        """
        # Convertir preguntas a formato JSON
        questions_json = []
        for question in batch.questions:
            question_dict = {
                "codigo_procedimiento": getattr(question, 'codigo_procedimiento', question.procedure_codigo),
                "version_proc": getattr(question, 'version_proc', int(question.procedure_version)),
                "version_preg": getattr(question, 'version_preg', 1),
                "prompt": getattr(question, 'prompt', "1.1"),
                "tipo_proc": getattr(question, 'tipo_proc', "TECNICO"),
                "puntaje_ia": getattr(question, 'puntaje_ia', 0),
                "puntaje_e1": getattr(question, 'puntaje_e1', 0),
                "puntaje_e2": getattr(question, 'puntaje_e2', 0),
                "puntaje_e3": getattr(question, 'puntaje_e3', 0),
                "puntaje_e4": getattr(question, 'puntaje_e4', 0),
                "comentario_e1": getattr(question, 'comentario_e1', ""),
                "comentario_e2": getattr(question, 'comentario_e2', ""),
                "comentario_e3": getattr(question, 'comentario_e3', ""),
                "comentario_e4": getattr(question, 'comentario_e4', ""),
                "pregunta": question.pregunta,
                "opciones": question.opciones,
                "historial_revision": getattr(question, 'historial_revision', [])
            }
            questions_json.append(question_dict)
        
        # Crear prompt
        prompt = f"""PROCEDIMIENTO T√âCNICO COMPLETO:
{procedure_text}

CONJUNTO DE CINCO PREGUNTAS EN FORMATO JSON:
{json.dumps(questions_json, indent=2, ensure_ascii=False)}

Eval√∫a cada una de las cinco preguntas seg√∫n tus criterios especializados."""
        
        return prompt

    def _create_fallback_batch_response(self, num_questions: int) -> List[Dict[str, Any]]:
        """Crear respuesta de fallback para un lote de preguntas"""
        evaluator_num = self._get_evaluator_number()
        fallback_responses = []
        
        for i in range(num_questions):
            response = {
                f"puntaje_e{evaluator_num}": 1,
                f"comentario_e{evaluator_num}": f"Validaci√≥n autom√°tica fallback para pregunta {i+1}"
            }
            fallback_responses.append(response)
        
        return fallback_responses

    def _validate_batch_response_structure(self, validation_data: List[Dict[str, Any]]) -> None:
        """
        Validar que la respuesta del validador tenga la estructura correcta para un lote
        """
        print(f"üîß Validando estructura de respuesta de lote para {self.validator_type.value}")
        
        if not isinstance(validation_data, list):
            raise ValueError(f"Se esperaba una lista, se recibi√≥: {type(validation_data)}")
        
        if len(validation_data) != 5:
            raise ValueError(f"Se esperaban 5 elementos, se recibieron: {len(validation_data)}")
        
        evaluator_num = self._get_evaluator_number()
        required_fields = [f"puntaje_e{evaluator_num}", f"comentario_e{evaluator_num}"]
        
        for i, item in enumerate(validation_data):
            print(f"   üîç Validando item {i+1}: {item}")
            
            if not isinstance(item, dict):
                raise ValueError(f"Item {i+1} debe ser un diccionario, se recibi√≥: {type(item)}")
            
            # Verificar campos requeridos (flexibilidad para diferentes formatos)
            score_field = None
            comment_field = None
            
            # Buscar campos de score
            for field in [f"puntaje_e{evaluator_num}", "score"]:
                if field in item:
                    score_field = field
                    break
            
            # Buscar campos de comentario  
            for field in [f"comentario_e{evaluator_num}", "comment"]:
                if field in item:
                    comment_field = field
                    break
            
            if not score_field:
                raise ValueError(f"Item {i+1} falta campo de puntaje")
            
            if not comment_field:
                raise ValueError(f"Item {i+1} falta campo de comentario")
            
            # Validar score
            score = item[score_field]
            if not isinstance(score, int) or score not in [0, 1]:
                raise ValueError(f"Item {i+1}: score debe ser 0 o 1, recibido: {score}")
            
            # Validar comment
            comment = item[comment_field]
            if not isinstance(comment, str):
                raise ValueError(f"Item {i+1}: comment debe ser string, recibido: {type(comment)}")
        
        print(f"‚úÖ Estructura de respuesta de lote v√°lida para {self.validator_type.value}")

    # Mantener m√©todo legacy para compatibilidad
    async def validate_question(self, question: QuestionInProcess) -> ValidationResult:
        """
        Validar una pregunta espec√≠fica (m√©todo legacy - mantener compatibilidad)
        """
        try:
            print(f"üîç Validando pregunta individual {question.id} con validador {self.validator_type.value}")
            
            # Preparar prompt para el validador
            question_prompt = self._prepare_question_prompt(question)
            
            # Realizar validaci√≥n
            validation_response = await self._call_validator_api(question_prompt)
            
            # Limpiar la respuesta antes de parsear JSON
            clean_response = self._clean_json_response(validation_response)
            
            # Parsear respuesta JSON limpia
            try:
                validation_data = json.loads(clean_response)
                print(f"‚úÖ JSON parseado exitosamente para {self.validator_type.value}")
            except json.JSONDecodeError as e:
                print(f"‚ùå Error parseando JSON limpio para {self.validator_type.value}: {e}")
                validation_data = self._extract_json_manually(clean_response)
            
            # Validar estructura de respuesta
            self._validate_response_structure(validation_data)
            
            # Crear resultado
            result = ValidationResult(
                validator_type=self.validator_type,
                score=validation_data["score"],
                comment=validation_data["comment"],
                timestamp=get_current_timestamp(),
                model_used=GENERATION_CONFIG["openai_model"]
            )
            
            if DEBUG_CONFIG["verbose_logging"]:
                print(f"   ‚úÖ Validaci√≥n completada: Score={result.score}, Comment='{result.comment[:50]}...'")
            
            return result
            
        except Exception as e:
            print(f"‚ùå Error en validador {self.validator_type.value}: {e}")
            
            # Crear resultado de error pero no cr√≠tico
            error_result = ValidationResult(
                validator_type=self.validator_type,
                score=1,  # Asumir v√°lido por defecto para no bloquear el flujo
                comment=f"Error en validaci√≥n autom√°tica: {str(e)[:100]}",
                timestamp=get_current_timestamp(),
                model_used="error_fallback"
            )
            
            # Si es un validador cr√≠tico, re-lanzar el error
            if self.config["critical"]:
                raise
            
            return error_result
    
    def _prepare_question_prompt(self, question: QuestionInProcess) -> str:
        """
        Preparar el prompt con la pregunta para enviar al validador
        """
        prompt = f"""
Pregunta a validar:
{question.pregunta}

Opciones:
A. {question.opciones[0]}
B. {question.opciones[1]}
C. {question.opciones[2]}
D. {question.opciones[3]}

Informaci√≥n del procedimiento:
- C√≥digo: {question.procedure_codigo}
- Versi√≥n: {question.procedure_version}

Por favor eval√∫a esta pregunta seg√∫n tus criterios especializados.
"""
        return prompt
    
    
    async def _call_validator_api(self, question_prompt: str) -> str:
        """
        Llamar a la API del validador con manejo de errores
        """
        # Modo debug con mock responses
        if DEBUG_CONFIG["mock_openai_calls"]:
            if DEBUG_CONFIG["verbose_logging"]:
                print(f"üß™ Usando respuesta mock para validador {self.validator_type.value}")
            await asyncio.sleep(0.5)  # Simular latencia
            return MOCK_RESPONSES["validator"]
        
        if not self.client:
            raise ValueError("Cliente OpenAI no inicializado y no estamos en modo mock")
        
        try:
            print(f"ü§ñ Realizando llamada a OpenAI para validador {self.validator_type.value}")
            print(f"   - Modelo: {GENERATION_CONFIG['openai_model']}")
            print(f"   - Timeout: {self.config['timeout']}s")
            print(f"   - Temperature: 0.1")
            print(f"   - Max tokens: 500")
            
            response = self.client.chat.completions.create(
                model=GENERATION_CONFIG["openai_model"],
                messages=[
                    {
                        "role": "system", 
                        "content": self.system_message
                    },
                    {
                        "role": "user",
                        "content": question_prompt
                    }
                ],
                temperature=0.4,  # Baja temperatura para validaci√≥n consistente
                max_tokens=500,   # Respuestas cortas
                timeout=self.config["timeout"]
            )
            
            content = response.choices[0].message.content
            if not content:
                raise ValueError(f"Validador {self.validator_type.value} retorn√≥ contenido vac√≠o")
            
            print(f"‚úÖ Respuesta recibida de OpenAI para {self.validator_type.value}")
            print(f"   - Longitud de respuesta: {len(content)} caracteres")
            print(f"   - Primeros 100 caracteres: {content[:100]}")
            
            return content
            
        except Exception as e:
            print(f"‚ùå Error llamando validador {self.validator_type.value}: {str(e)}")
            print(f"üìä Tipo de error: {type(e).__name__}")
            raise Exception(f"Error llamando validador {self.validator_type.value}: {str(e)}")
            
        except Exception as e:
            print(f"‚ùå Error llamando validador {self.validator_type.value}: {str(e)}")
            # Usar respuesta mock como fallback
            return MOCK_RESPONSES["validator"]
    
    def _validate_response_structure(self, validation_data: Dict[str, Any]) -> None:
        """
        Validar que la respuesta del validador tenga la estructura correcta
        """
        print(f"üîß Validando estructura de respuesta para {self.validator_type.value}")
        print(f"   - Datos recibidos: {validation_data}")
        print(f"   - Tipo de datos: {type(validation_data)}")
        
        required_fields = ["score", "comment"]
        
        for field in required_fields:
            if field not in validation_data:
                print(f"‚ùå Campo faltante: {field}")
                raise ValueError(f"Campo requerido faltante en respuesta del validador: {field}")
            else:
                print(f"‚úÖ Campo presente: {field} = {validation_data[field]}")
        
        # Validar score
        score = validation_data["score"]
        print(f"üîç Validando score: {score} (tipo: {type(score)})")
        if not isinstance(score, int) or score not in [0, 1]:
            print(f"‚ùå Score inv√°lido: {score}")
            raise ValueError(f"Score debe ser 0 o 1, recibido: {score}")
        
        # Validar comment
        comment = validation_data["comment"]
        print(f"üîç Validando comment: '{comment}' (tipo: {type(comment)})")
        if not isinstance(comment, str) or not comment.strip():
            print(f"‚ùå Comment inv√°lido: {comment}")
            raise ValueError("Comment debe ser un string no vac√≠o")
        
        print(f"‚úÖ Estructura de respuesta v√°lida para {self.validator_type.value}")

class ValidationEngine:
    """
    Motor de validaci√≥n que orquesta m√∫ltiples validadores
    """
    
    def __init__(self):
        """
        Inicializar motor de validaci√≥n con todos los validadores habilitados
        """
        self.validators = {}
        enabled_validators = get_enabled_validators()
        
        print(f"üîß Inicializando ValidationEngine...")
        print(f"   - Validadores habilitados: {enabled_validators}")
        
        # Verificar API key antes de crear validadores
        api_key = get_openai_api_key()
        if not api_key and not DEBUG_CONFIG["mock_openai_calls"]:
            print("‚ö†Ô∏è No hay API Key configurada, activando modo mock para validadores")
            DEBUG_CONFIG["mock_openai_calls"] = True
        
        # Crear instancias de validadores
        for validator_name in enabled_validators:
            try:
                validator_type = ValidatorType(validator_name)
                self.validators[validator_name] = QuestionValidator(validator_type)
            except Exception as e:
                print(f"‚ö†Ô∏è Error inicializando validador {validator_name}: {e}")
                # No detener el proceso, continuar sin este validador
                continue
        
        if not self.validators:
            print("‚ö†Ô∏è No se pudo inicializar ning√∫n validador, activando modo mock")
            DEBUG_CONFIG["mock_openai_calls"] = True
            # Crear validadores en modo mock
            for validator_name in enabled_validators:
                try:
                    validator_type = ValidatorType(validator_name)
                    self.validators[validator_name] = QuestionValidator(validator_type)
                except Exception as e:
                    print(f"‚ùå Error inicializando validador {validator_name} en modo mock: {e}")
                    continue
        
        print(f"‚úÖ ValidationEngine inicializado con {len(self.validators)} validadores")
    
    async def validate_question_with_all_validators(self, question: QuestionInProcess) -> QuestionInProcess:
        """
        Validar una pregunta con todos los validadores habilitados
        
        Args:
            question: Pregunta a validar
            
        Returns:
            Pregunta actualizada con resultados de validaci√≥n
        """
        print(f"üîç Iniciando validaci√≥n completa para pregunta {question.id}")
        
        # Limpiar validaciones previas
        question.validations = []
        question.status = QuestionStatus.validating
        question.updated_at = get_current_timestamp()
        
        validation_results = []
        total_score = 0
        total_weight = 0
        
        # Ejecutar todos los validadores
        for validator_name, validator in self.validators.items():
            try:
                print(f"   üîç Ejecutando validador: {validator_name}")
                print(f"   üìä Configuraci√≥n del validador:")
                print(f"      - Peso: {VALIDATORS_CONFIG[validator_name]['weight']}")
                print(f"      - Cr√≠tico: {VALIDATORS_CONFIG[validator_name]['critical']}")
                print(f"      - Timeout: {VALIDATORS_CONFIG[validator_name]['timeout']}s")
                
                result = await validator.validate_question(question)
                validation_results.append(result)
                
                # Calcular score ponderado
                weight = VALIDATORS_CONFIG[validator_name]["weight"]
                total_score += result.score * weight
                total_weight += weight
                
                print(f"   ‚úÖ {validator_name}: Score={result.score}, Weight={weight}")
                print(f"      Comment: {result.comment}")
                print(f"      Model used: {result.model_used}")
                print(f"      Timestamp: {result.timestamp}")
                
            except Exception as e:
                print(f"   ‚ùå Error en validador {validator_name}: {e}")
                print(f"   üìä Stack trace:")
                import traceback
                traceback.print_exc()
                
                # Si es cr√≠tico, detener validaci√≥n
                if VALIDATORS_CONFIG[validator_name]["critical"]:
                    question.status = QuestionStatus.failed
                    question.updated_at = get_current_timestamp()
                    print(f"üõë Validador cr√≠tico {validator_name} fall√≥, deteniendo validaci√≥n")
                    raise Exception(f"Validador cr√≠tico {validator_name} fall√≥: {e}")
        
        # Calcular score promedio ponderado
        average_score = total_score / total_weight if total_weight > 0 else 0
        
        print(f"üìä RESUMEN DE VALIDACI√ìN:")
        print(f"   - Total validadores ejecutados: {len(validation_results)}")
        print(f"   - Score total ponderado: {total_score}")
        print(f"   - Peso total: {total_weight}")
        print(f"   - Score promedio: {average_score:.2f}")
        print(f"   - Umbral requerido: {VALIDATION_THRESHOLD}")
        
        # Actualizar pregunta con resultados
        question.validations = validation_results
        question.updated_at = get_current_timestamp()
        
        # Determinar estado basado en threshold
        if average_score >= VALIDATION_THRESHOLD:
            question.status = QuestionStatus.completed
            print(f"   ‚úÖ Pregunta aprobada: Score promedio = {average_score:.2f}")
        else:
            question.status = QuestionStatus.needs_correction
            print(f"   ‚ö†Ô∏è Pregunta necesita correcci√≥n: Score promedio = {average_score:.2f}")
        
        return question
    
    async def validate_batch(self, batch: QuestionBatch, procedure_text: str = "") -> QuestionBatch:
        """
        Validar todas las preguntas de un lote usando nueva l√≥gica de batch
        
        Args:
            batch: Lote de preguntas a validar
            procedure_text: Texto completo del procedimiento t√©cnico
        """
        print(f"üîç Iniciando validaci√≥n de lote {batch.batch_id} con nueva l√≥gica de batch")
        print(f"   - Preguntas a validar: {len(batch.questions)}")
        print(f"   - Procedimiento provisto: {len(procedure_text)} caracteres")
        
        batch.status = ProcedureStatus.validating
        batch.updated_at = get_current_timestamp()
        
        # Ejecutar todos los validadores en paralelo con el lote completo
        all_validation_results = {}
        
        for validator_name, validator in self.validators.items():
            try:
                print(f"   üîç Ejecutando validador de lote: {validator_name}")
                
                # Validar lote completo con procedimiento
                batch_results = await validator.validate_batch(batch, procedure_text)
                all_validation_results[validator_name] = batch_results
                
                print(f"   ‚úÖ {validator_name}: {len(batch_results)} resultados obtenidos")
                
            except Exception as e:
                print(f"   ‚ùå Error en validador de lote {validator_name}: {e}")
                
                # Si es cr√≠tico, detener validaci√≥n
                if VALIDATORS_CONFIG[validator_name]["critical"]:
                    batch.status = ProcedureStatus.failed
                    batch.updated_at = get_current_timestamp()
                    print(f"üõë Validador cr√≠tico {validator_name} fall√≥, deteniendo validaci√≥n")
                    raise Exception(f"Validador cr√≠tico {validator_name} fall√≥: {e}")
                
                # Crear resultados de fallback
                evaluator_num = validator._get_evaluator_number()
                fallback_results = []
                for i in range(len(batch.questions)):
                    fallback_result = {
                        f"puntaje_e{evaluator_num}": 1,
                        f"comentario_e{evaluator_num}": f"Error en validador: {str(e)[:50]}"
                    }
                    fallback_results.append(fallback_result)
                all_validation_results[validator_name] = fallback_results
        
        # Aplicar resultados de validaci√≥n a cada pregunta
        for i, question in enumerate(batch.questions):
            # Aplicar resultados de todos los validadores a esta pregunta
            for validator_name, results in all_validation_results.items():
                if i < len(results):
                    result = results[i]
                    # Actualizar campos de la pregunta
                    for key, value in result.items():
                        setattr(question, key, value)
            
            # Actualizar estado de la pregunta
            question.status = QuestionStatus.completed  # Asumir completado por defecto
            question.updated_at = get_current_timestamp()
            
            print(f"   ‚úÖ Pregunta {i+1} actualizada con resultados de validaci√≥n")
        
        # Calcular score promedio del lote
        total_score = 0
        total_validations = 0
        
        for question in batch.questions:
            question_score = 0
            question_validations = 0
            
            # Sumar scores de todos los evaluadores
            for evaluator_num in range(1, 5):  # e1, e2, e3, e4
                score_field = f"puntaje_e{evaluator_num}"
                if hasattr(question, score_field):
                    score = getattr(question, score_field, 0)
                    question_score += score
                    question_validations += 1
            
            if question_validations > 0:
                avg_question_score = question_score / question_validations
                total_score += avg_question_score
                total_validations += 1
        
        batch.validation_score = total_score / total_validations if total_validations > 0 else 1.0
        batch.updated_at = get_current_timestamp()
        
        # Determinar estado del lote
        batch.status = ProcedureStatus.completed  # Asumir completado si llegamos aqu√≠
        
        print(f"‚úÖ Validaci√≥n de lote completada:")
        print(f"   - Total preguntas: {len(batch.questions)}")
        print(f"   - Score promedio del lote: {batch.validation_score:.2f}")
        print(f"   - Validadores ejecutados: {len(all_validation_results)}")
        
        return batch
    
    def _calculate_question_score(self, validations: List[ValidationResult]) -> float:
        """
        Calcular score ponderado de una pregunta basado en sus validaciones
        """
        total_score = 0
        total_weight = 0
        
        for validation in validations:
            validator_name = validation.validator_type.value
            if validator_name in VALIDATORS_CONFIG:
                weight = VALIDATORS_CONFIG[validator_name]["weight"]
                total_score += validation.score * weight
                total_weight += weight
        
        return total_score / total_weight if total_weight > 0 else 0
    
    def get_validation_summary(self, batch: QuestionBatch) -> Dict[str, Any]:
        """
        Obtener resumen de validaci√≥n para un lote
        """
        if not batch.questions:
            return {"error": "No hay preguntas en el lote"}
        
        # Estad√≠sticas generales
        total_questions = len(batch.questions)
        completed = sum(1 for q in batch.questions if q.status == QuestionStatus.completed)
        needs_correction = sum(1 for q in batch.questions if q.status == QuestionStatus.needs_correction)
        failed = sum(1 for q in batch.questions if q.status == QuestionStatus.failed)
        
        # Estad√≠sticas por validador
        validator_stats = {}
        for validator_name in self.validators.keys():
            scores = []
            for question in batch.questions:
                for validation in question.validations:
                    if validation.validator_type.value == validator_name:
                        scores.append(validation.score)
            
            if scores:
                validator_stats[validator_name] = {
                    "average_score": sum(scores) / len(scores),
                    "total_evaluations": len(scores),
                    "pass_rate": sum(scores) / len(scores)
                }
        
        # Problemas m√°s comunes
        common_issues = []
        for question in batch.questions:
            for validation in question.validations:
                if validation.score == 0:
                    common_issues.append(f"{validation.validator_type.value}: {validation.comment}")
        
        summary = {
            "batch_id": batch.batch_id,
            "procedure_codigo": batch.procedure_codigo,
            "total_questions": total_questions,
            "validation_results": {
                "completed": completed,
                "needs_correction": needs_correction,
                "failed": failed,
                "success_rate": completed / total_questions if total_questions > 0 else 0
            },
            "overall_score": batch.validation_score,
            "validator_statistics": validator_stats,
            "common_issues": common_issues[:10],  # Top 10 issues
            "timestamp": get_current_timestamp()
        }
        
        return summary

# =============================================================================
# FUNCIONES DE UTILIDAD
# =============================================================================

def create_validation_engine() -> ValidationEngine:
    """
    Crear instancia del motor de validaci√≥n
    """
    return ValidationEngine()

async def validate_single_question(
    question: QuestionInProcess,
    validator_types: Optional[List[ValidatorType]] = None
) -> QuestionInProcess:
    """
    Funci√≥n de conveniencia para validar una sola pregunta
    """
    engine = create_validation_engine()
    
    # Si se especifican validadores espec√≠ficos, filtrar
    if validator_types:
        original_validators = engine.validators
        engine.validators = {
            name: validator for name, validator in original_validators.items()
            if ValidatorType(name) in validator_types
        }
    
    return await engine.validate_question(question)

def enable_debug_validation():
    """Habilitar modo debug para validaci√≥n"""
    DEBUG_CONFIG["enabled"] = True
    DEBUG_CONFIG["mock_openai_calls"] = True
    DEBUG_CONFIG["verbose_logging"] = True
    print("üß™ Modo debug de validaci√≥n habilitado")

def disable_debug_validation():
    """Deshabilitar modo debug para validaci√≥n"""
    DEBUG_CONFIG["enabled"] = False
    DEBUG_CONFIG["mock_openai_calls"] = False
    DEBUG_CONFIG["verbose_logging"] = False
    print("üîß Modo debug de validaci√≥n deshabilitado")

# =============================================================================
# TESTING
# =============================================================================

async def test_validation():
    """
    Funci√≥n de testing para el sistema de validaci√≥n
    """
    print("üß™ Testing ValidationEngine...")
    
    # Habilitar modo debug
    enable_debug_validation()
    
    # Verificar configuraci√≥n
    from .config import validate_admin_config
    if not validate_admin_config():
        print("‚ùå Configuraci√≥n inv√°lida")
        return
    
    try:
        # Crear pregunta de prueba
        test_question = QuestionInProcess(
            id="test_q1",
            procedure_codigo="TEST-001", 
            procedure_version="1",
            pregunta="¬øCu√°l es el primer paso en el procedimiento de prueba?",
            opciones=[
                "Verificar condiciones iniciales",
                "Iniciar operaci√≥n directamente",
                "Contactar supervisor", 
                "Revisar documentaci√≥n"
            ],
            status=QuestionStatus.generated,
            created_at=get_current_timestamp(),
            updated_at=get_current_timestamp()
        )
        
        print(f"üìù Pregunta de prueba creada: {test_question.pregunta}")
        
        # Crear motor de validaci√≥n
        engine = create_validation_engine()
        
        # Validar pregunta
        validated_question = await engine.validate_question(test_question)
        
        print(f"‚úÖ Validaci√≥n completada!")
        print(f"   - Estado: {validated_question.status}")
        print(f"   - Validaciones realizadas: {len(validated_question.validations)}")
        
        # Mostrar resultados de cada validador
        for validation in validated_question.validations:
            print(f"   - {validation.validator_type.value}: Score={validation.score}")
            print(f"     Comentario: {validation.comment}")
        
        # Crear lote de prueba
        test_batch = QuestionBatch(
            batch_id="test_batch_001",
            procedure_codigo="TEST-001",
            procedure_version="1", 
            procedure_name="Procedimiento de Prueba",
            questions=[validated_question],
            status=ProcedureStatus.generating,
            created_at=get_current_timestamp(),
            updated_at=get_current_timestamp()
        )
        
        # Obtener resumen
        summary = engine.get_validation_summary(test_batch)
        print(f"üìä Resumen de validaci√≥n:")
        print(f"   - Success rate: {summary['validation_results']['success_rate']:.2f}")
        print(f"   - Overall score: {summary['overall_score']:.2f}")
        
    except Exception as e:
        print(f"‚ùå Error en test de validaci√≥n: {e}")
        if DEBUG_CONFIG["verbose_logging"]:
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    import asyncio
    asyncio.run(test_validation())